{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 预处理你提供的原始问答数据集 `qa_data.jsonl`, 里面每条数据一般大致由问题和参考答案组成。**这部分的预处理逻辑一般要自己编写，处理后使其满足每条数据只有两个属性： `\"question\"`,  `\"right_answer\"`，分别对应问题、参考答案。** 输出文件 `qa_data_ready.jsonl` 。可参考下面的实现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "with open('/byllm/qa_data_ready.jsonl','w',encoding='utf-8') as f2:\n",
    "    with open('/byllm/qa_data.jsonl','r',encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "        all_q = total_lines\n",
    "        f.seek(0)\n",
    "        for line in tqdm(f, total=total_lines, desc=\"Processing lines\"):\n",
    "            item = json.loads(line.strip())\n",
    "            q=item['knowledge'] + ' so ' + item['question']\n",
    "            \n",
    "            my_dict={\"question\":q,\"right_answer\":item['answer']}\n",
    "            \n",
    "            f2.write(json.dumps(my_dict)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 加载需要评测的模型，这里评测`Qwen1.5-0.5B-Chat`。定义批量回答函数 `batch_inference`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "modelpath=\"Qwen1.5-0.5B-Chat\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "   modelpath,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath,trust_remote_code=True, padding_side=\"left\")\n",
    "def batch_inference(prompts:list[str])->list[str]:\n",
    "    texts=[]\n",
    "    for prompt in prompts:\n",
    "        messages = [\n",
    "            # 有的模型可以省略system prompt\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts.append(text)\n",
    "    model_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "       \n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 现在开始调用上述的模型对`qa_data_ready.jsonl`里的问题`question`批量回答。输出文件`qa_data_answer.jsonl`, 每条数据有三个属性 `\"question\"`,  `\"your_answer\"`, `\"right_answer\"`。`\"your_answer\"`就是模型的回答。批次大小`batchsize`根据自己的GPU资源多少进行调整，越大越快。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "batch_size=50\n",
    "\n",
    "eval_dataset = load_dataset(\"/byllm\", data_files=\"/byllm/qa_data_ready.jsonl\", split=\"train\")\n",
    "data_loader = DataLoader(dataset=eval_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "with open('/byllm/qa_data_answer.jsonl','w',encoding='utf-8') as f2:\n",
    "    for i in tqdm(data_loader, total=len(data_loader)):\n",
    "        ans = batch_inference(i['question'])\n",
    "        for q,a,ra in zip(i['question'], ans, i['right_answer']):\n",
    "            my_dict={\"question\":q,\"your_answer\":a,\"right_answer\":ra }\n",
    "            f2.write(json.dumps(my_dict)+'\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 现在调用网络大模型API对 `qa_data_answer.jsonl` 里你模型的回答和参考答案进行对比，判断是否一致。若某条数据一致，输出的属性`\"label\"`是1，不一致是0。最终输出打分文件`qa_data_answer_judge.jsonl`，里面有4个属性：`\"question\"`,  `\"your_answer\"`, `\"right_answer\"`, `\"label\"`,`\"response\"`，并输出正确率。这里调用的大模型API是[deepseek深度求索](https://platform.deepseek.com/api_keys)。点击链接可以申请API，并在代码中进行替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "#这里输入你申请的key\n",
    "api_key='sk-af2903a7da03f06dddbnwaubda'\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key, base_url='https://api.deepseek.com'\n",
    ")\n",
    "\n",
    "\n",
    "def func(s):\n",
    "    # get a string, return a answer string\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": s},\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "        temperature=0.7,\n",
    "        stream=False,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def llm_judge(answer_file, judge_func):\n",
    "    # answer_file: a jsonl file, each line with \"question\", \"your_answer\", \"right_answer\".\n",
    "    # judge_func: get a string, return a answer string\n",
    "    judge_file = answer_file[:-6] + \"_judge.jsonl\"\n",
    "    all_q = 0\n",
    "    right_a = 0\n",
    "    template = \"Now I give you one question and two answers to it. One of the answers is student's answer, another is the right answer. Please based on the given right answer, judge if the student's answer get\\\n",
    "        it right. If the student get it right, please respond with a 'yes' and reasons, otherwise with a 'no' and reasons.\\n Here is the question:{question}.\\n \\\n",
    "            Student's answer: {your_answer}. \\n Right answer: {right_answer}. \"\n",
    "\n",
    "    with open(judge_file, \"w\", encoding=\"utf-8\") as f2:\n",
    "\n",
    "        with open(answer_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            total_lines = sum(1 for _ in f)\n",
    "            f.seek(0)\n",
    "            for line in tqdm(f, total=total_lines, desc=\"Processing lines\"):\n",
    "                item = json.loads(line.strip())\n",
    "                pro = template.format(\n",
    "                    question=item[\"question\"],\n",
    "                    your_answer=item[\"your_answer\"],\n",
    "                    right_answer=item[\"right_answer\"],\n",
    "                )\n",
    "                try:\n",
    "                    response= judge_func(pro)\n",
    "                except Exception:\n",
    "                    # 若模型拒绝回答就丢弃这条数据\n",
    "                    continue\n",
    "                label = 0\n",
    "                # 若开始几个字符包括yes就是yes，否则视为no\n",
    "                if \"yes\" in response.lower()[:5]:\n",
    "                    right_a += 1\n",
    "                    label = 1\n",
    "\n",
    "                result = {\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"your_answer\": item[\"your_answer\"],\n",
    "                    \"right_answer\": item[\"right_answer\"],\n",
    "                    \"label\": label,\n",
    "                    \"response\":response\n",
    "                }\n",
    "                f2.write(json.dumps(result) + \"\\n\")\n",
    "                all_q += 1\n",
    "               \n",
    "    return right_a, all_q, right_a / all_q\n",
    "\n",
    "\n",
    "right_a, all_q, accuracy = llm_judge(\n",
    "    \"byllm/qa_data_answer.jsonl\", func\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"right answers ={right_a}, all = {all_q}, accuracy ={accuracy} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "一次针对 QWEN 0.5B 的简单评测就完成了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
