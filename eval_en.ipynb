{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pre-process your QA dataset `qa_data.jsonl`, of which each item roughly include question and answer. Preprocess it into a new dataset `qa_data_ready.jsonl` of which each item only have two properties ： `\"question\"`,  `\"right_answer\"`. You can refer to the following for implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "with open('/byllm/qa_data_ready.jsonl','w',encoding='utf-8') as f2:\n",
    "    with open('/byllm/qa_data.jsonl','r',encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "        all_q = total_lines\n",
    "        f.seek(0)\n",
    "        for line in tqdm(f, total=total_lines, desc=\"Processing lines\"):\n",
    "            item = json.loads(line.strip())\n",
    "            q=item['knowledge'] + ' so ' + item['question']\n",
    "            \n",
    "            my_dict={\"question\":q,\"right_answer\":item['answer']}\n",
    "            \n",
    "            f2.write(json.dumps(my_dict)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load your model for evaluation. Here I load `Qwen1.5-0.5B-Chat`. Define `batch_inference`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "modelpath=\"Qwen1.5-0.5B-Chat\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "   modelpath,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath,trust_remote_code=True, padding_side=\"left\")\n",
    "def batch_inference(prompts:list[str])->list[str]:\n",
    "    texts=[]\n",
    "    for prompt in prompts:\n",
    "        messages = [\n",
    "            # 有的模型可以省略system prompt\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts.append(text)\n",
    "    model_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "       \n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now your model batchly answer the questions in `qa_data_ready.jsonl`. Output will be `qa_data_answer.jsonl`, which has the properties: `\"question\"`,  `\"your_answer\"`(generated by your model), `\"right_answer\"`. Adjust the `batch_size` to your need. The bigger, the faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# batchsize根据自己的GPU资源多少进行调整，越大越快。\n",
    "batch_size=50\n",
    "\n",
    "eval_dataset = load_dataset(\"/byllm\", data_files=\"/byllm/qa_data_ready.jsonl\", split=\"train\")\n",
    "data_loader = DataLoader(dataset=eval_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "with open('/byllm/qa_data_answer.jsonl','w',encoding='utf-8') as f2:\n",
    "    for i in tqdm(data_loader, total=len(data_loader)):\n",
    "        ans = batch_inference(i['question'])\n",
    "        for q,a,ra in zip(i['question'], ans, i['right_answer']):\n",
    "            my_dict={\"question\":q,\"your_answer\":a,\"right_answer\":ra }\n",
    "            f2.write(json.dumps(my_dict)+'\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now call a online LLM to judge your model's answers base on right answers, in `qa_data_answer.jsonl`. If your answer is considered by the online LLM to be right, a `\"label\"` is to 1，otherwise 0.Output is `qa_data_answer_judge.jsonl`，which has the properties: `\"question\"`, `\"your_answer\"`, `\"right_answer\"`, `\"label\"`, `\"response\"`. The online LLM here is [deepseek深度求索](https://platform.deepseek.com/api_keys). Click the link to apply for your API key, and replace the `api_key` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Here please replace with your API key\n",
    "api_key='sk-af2903a7da03f06dddbnwaubda'\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key, base_url='https://api.deepseek.com'\n",
    ")\n",
    "\n",
    "\n",
    "def func(s):\n",
    "    # get a string, return a answer string\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": s},\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "        temperature=0.7,\n",
    "        stream=False,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def llm_judge(answer_file, judge_func):\n",
    "    # answer_file: a jsonl file, each line with \"question\", \"your_answer\", \"right_answer\".\n",
    "    # judge_func: get a string, return a answer string\n",
    "    judge_file = answer_file[:-6] + \"_judge.jsonl\"\n",
    "    all_q = 0\n",
    "    right_a = 0\n",
    "    template = \"Now I give you one question and two answers to it. One of the answers is student's answer, another is the right answer. Please based on the given right answer, judge if the student's answer get\\\n",
    "        it right. If the student get it right, please respond with a 'yes' and reasons, otherwise with a 'no' and reasons.\\n Here is the question:{question}.\\n \\\n",
    "            Student's answer: {your_answer}. \\n Right answer: {right_answer}. \"\n",
    "\n",
    "    with open(judge_file, \"w\", encoding=\"utf-8\") as f2:\n",
    "\n",
    "        with open(answer_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            total_lines = sum(1 for _ in f)\n",
    "            f.seek(0)\n",
    "            for line in tqdm(f, total=total_lines, desc=\"Processing lines\"):\n",
    "                item = json.loads(line.strip())\n",
    "                pro = template.format(\n",
    "                    question=item[\"question\"],\n",
    "                    your_answer=item[\"your_answer\"],\n",
    "                    right_answer=item[\"right_answer\"],\n",
    "                )\n",
    "                try:\n",
    "                    response= judge_func(pro)\n",
    "                except Exception:\n",
    "                    # abandon this item if rejected\n",
    "                    continue\n",
    "                label = 0\n",
    "                # see as a yes only if the first 5 chars include 'yes'\n",
    "                if \"yes\" in response.lower()[:5]:\n",
    "                    right_a += 1\n",
    "                    label = 1\n",
    "\n",
    "                result = {\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"your_answer\": item[\"your_answer\"],\n",
    "                    \"right_answer\": item[\"right_answer\"],\n",
    "                    \"label\": label,\n",
    "                    \"response\":response\n",
    "                }\n",
    "                f2.write(json.dumps(result) + \"\\n\")\n",
    "                all_q += 1\n",
    "               \n",
    "    return right_a, all_q, right_a / all_q\n",
    "\n",
    "\n",
    "right_a, all_q, accuracy = llm_judge(\n",
    "    \"byllm/qa_data_answer.jsonl\", func\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"right answers ={right_a}, all = {all_q}, accuracy ={accuracy} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A simple evaluation against Qwen 0.5B is done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
